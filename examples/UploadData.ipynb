{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f56a9b3",
   "metadata": {},
   "source": [
    "# Upstream Data Upload Guide\n",
    "\n",
    "## Overview\n",
    "\n",
    "This guide demonstrates how to authenticate with the Upstream API and upload sensor data using CSV files for environmental monitoring campaigns.\n",
    "\n",
    "## What You Can Do\n",
    "\n",
    "The Upstream API allows you to:\n",
    "- Authenticate and obtain access tokens\n",
    "- Upload sensor definitions and measurement data\n",
    "- Manage environmental monitoring campaigns\n",
    "- Query and retrieve measurement data\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Valid Upstream account credentials\n",
    "- Python 3.7+ with `requests` library installed\n",
    "- CSV files with sensor and measurement data formatted correctly\n",
    "\n",
    "## Installation\n",
    "\n",
    "```bash\n",
    "pip install requests\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dee1efa",
   "metadata": {},
   "source": [
    "## Quick Start\n",
    "\n",
    "1. **Authenticate** with the API to get your access token\n",
    "2. **Prepare your CSV files** following the required format\n",
    "3. **Upload your data** using the provided functions\n",
    "4. **Monitor the results** and verify successful upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3de5ed4d-505a-4a59-b15a-7de41e8246d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "import json\n",
    "import getpass\n",
    "import os\n",
    "from typing import Dict, Any, Optional, List\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65443e09",
   "metadata": {},
   "source": [
    "## 1. Authentication\n",
    "\n",
    "First, we need to authenticate with the Upstream API to obtain an access token.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b250831-bec9-4425-b165-127e49d76ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Upstream API Authentication ===\n",
      "✅ Authentication successful!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def authenticate_upstream(base_url: str = \"https://upstream-dso.tacc.utexas.edu/dev\") -> str:\n",
    "    \"\"\"\n",
    "    Authenticate with Upstream API and return access token.\n",
    "    \n",
    "    Args:\n",
    "        base_url: Base URL for the Upstream API (dev or prod)\n",
    "        \n",
    "    Returns:\n",
    "        Access token string\n",
    "        \n",
    "    Raises:\n",
    "        Exception: If authentication fails\n",
    "    \"\"\"\n",
    "    auth_url = f\"{base_url}/api/v1/token\"\n",
    "    \n",
    "    print(\"=== Upstream API Authentication ===\")\n",
    "    credentials = {\n",
    "        \"username\": input(\"Username: \"),\n",
    "        \"password\": getpass.getpass(\"Password: \")\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(auth_url, data=credentials)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        token = response.json().get(\"access_token\")\n",
    "        if not token:\n",
    "            raise Exception(\"No access token in response\")\n",
    "            \n",
    "        print(\"✅ Authentication successful!\")\n",
    "        return token\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        raise Exception(f\"Authentication failed: {e}\")\n",
    "\n",
    "# Get authentication token\n",
    "token = authenticate_upstream()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9c608a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_authenticated_request(\n",
    "    method: str,\n",
    "    url: str,\n",
    "    token: str,\n",
    "    json: Optional[Dict] = None,\n",
    "    files: Optional[Dict] = None,\n",
    "    params: Optional[Dict] = None\n",
    ") -> requests.Response:\n",
    "    \"\"\"\n",
    "    Make an authenticated HTTP request to the Upstream API.\n",
    "    \n",
    "    Args:\n",
    "        method: HTTP method (GET, POST, PUT, DELETE, etc.)\n",
    "        url: Full URL for the request\n",
    "        token: Authentication token\n",
    "        json: JSON data for the request body\n",
    "        files: Files for multipart upload\n",
    "        params: URL parameters\n",
    "        \n",
    "    Returns:\n",
    "        Response object from the request\n",
    "        \n",
    "    Raises:\n",
    "        requests.exceptions.HTTPError: If the request fails\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {token}\",\n",
    "    }\n",
    "    \n",
    "    # Don't set Content-Type for file uploads (requests will set it automatically)\n",
    "    if files is None:\n",
    "        headers[\"Content-Type\"] = \"application/json\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.request(\n",
    "            method=method.upper(),\n",
    "            url=url,\n",
    "            headers=headers,\n",
    "            json=json,\n",
    "            files=files,\n",
    "            params=params,\n",
    "            timeout=300  # 5 minute timeout for large file uploads\n",
    "        )\n",
    "        \n",
    "        # Raise an exception for bad status codes\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        return response\n",
    "        \n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f\"❌ HTTP Error: {e}\")\n",
    "        print(f\"Response content: {response.text}\")\n",
    "        raise\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"❌ Request Error: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede83720",
   "metadata": {},
   "source": [
    "## 2. Helper Functions for API Requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a20206c-dc87-4f0d-b9cf-87923998a9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_campaign(\n",
    "     campaign_data:str,\n",
    "    \n",
    "    token: str,\n",
    "    base_url: str = \"https://upstream-dso.tacc.utexas.edu/dev\"\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Create a new campaign.\n",
    "    \n",
    "    Args:\n",
    "        name: Campaign name\n",
    "        description: Campaign description\n",
    "        allocation: TACC allocation identifier (required)\n",
    "        token: Authentication token\n",
    "        base_url: Base URL for the API\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing the created campaign data with ID\n",
    "    \"\"\"\n",
    "    url = f\"{base_url}/api/v1/campaigns\"    \n",
    "    response = make_authenticated_request(\n",
    "        method=\"POST\",\n",
    "        url=url,\n",
    "        token=token,\n",
    "        json=campaign_data\n",
    "    )\n",
    "    \n",
    "    result = response.json()\n",
    "    print(f\"✅ Campaign created successfully!\")\n",
    "    print(f\"Campaign ID: {result.get('id')}\")\n",
    "\n",
    "    \n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f94d96",
   "metadata": {},
   "source": [
    "### Creating Campaigns\n",
    "\n",
    "Before uploading CSV data, you need to create a campaign to organize your data collection project. A campaign serves as the top-level container for all related monitoring activities.\n",
    "\n",
    "#### Campaign Requirements\n",
    "\n",
    "**Required Fields:**\n",
    "- `name`: Descriptive name for your data collection project\n",
    "- `description`: Detailed description of the campaign's purpose and scope\n",
    "\n",
    "#### Campaign Best Practices\n",
    "\n",
    "🎯 **Naming Conventions:**\n",
    "- Use descriptive, unique names that clearly identify the project\n",
    "- Include dates, locations, or project codes for easy identification\n",
    "- Examples: \"Austin Air Quality 2024\", \"Hurricane Harvey Recovery Monitoring\"\n",
    "\n",
    "📝 **Descriptions:**\n",
    "- Provide detailed context about the campaign's objectives\n",
    "- Include information about duration, scope, and expected outcomes\n",
    "- Mention any relevant research or operational goals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e618b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Creating Campaign from Configuration ===\n",
      "📋 Campaign Configuration Summary:\n",
      "  Name: Beaumont Stream Gauge\n",
      "  Description: Beaumont Stream Gauge Campaign...\n",
      "✅ Campaign created successfully!\n",
      "Campaign ID: 10\n",
      "\n",
      "🎉 Campaign setup complete!\n",
      "Campaign ID: 10\n"
     ]
    }
   ],
   "source": [
    "def load_and_create_campaign(\n",
    "    config_path: str = \"campaigns/campaign.json\",\n",
    "    token: str = None,\n",
    "    base_url: str = \"https://upstream-dso.tacc.utexas.edu/dev\"\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Load campaign configuration from JSON and create the campaign.\n",
    "    \n",
    "    Args:\n",
    "        config_path: Path to the campaign configuration JSON file\n",
    "        token: Authentication token\n",
    "        base_url: Base URL for the API\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing the created campaign data with ID\n",
    "    \"\"\"\n",
    "    # Load configuration\n",
    "    with open(config_path) as campaign_data:\n",
    "        campaign_json = json.loads(campaign_data.read())\n",
    "\n",
    "    # Validate required fields\n",
    "    required_fields = [\"name\", \"description\"]\n",
    "    for field in required_fields:\n",
    "        if field not in campaign_json:\n",
    "            raise ValueError(f\"Missing required field '{field}' in campaign config\")\n",
    "    \n",
    "    # Display configuration summary\n",
    "    print(f\"📋 Campaign Configuration Summary:\")\n",
    "    print(f\"  Name: {campaign_json['name']}\")\n",
    "    print(f\"  Description: {campaign_json['description'][:100]}...\")\n",
    "    \n",
    "    if \"metadata\" in campaign_json:\n",
    "        metadata = campaign_json[\"metadata\"]\n",
    "        print(f\"  Project Lead: {metadata.get('project_lead', 'N/A')}\")\n",
    "        print(f\"  Institution: {metadata.get('institution', 'N/A')}\")\n",
    "    \n",
    "    # Create the campaign\n",
    "    campaign = create_campaign(\n",
    "        campaign_data=campaign_json,\n",
    "        token=token,\n",
    "        base_url=base_url\n",
    "    )\n",
    "    return campaign\n",
    "\n",
    "# Usage example\n",
    "print(\"=== Creating Campaign from Configuration ===\")\n",
    "try:\n",
    "    campaign = load_and_create_campaign(\n",
    "        config_path=\"campaigns/campaign.json\",\n",
    "        token=token\n",
    "    )\n",
    "    \n",
    "    campaign_id = campaign['id']\n",
    "    print(f\"\\n🎉 Campaign setup complete!\")\n",
    "    print(f\"Campaign ID: {campaign_id}\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"❌ Configuration file error: {e}\")\n",
    "    print(\"💡 Please create a campaigns/campaign.json file with your campaign details\")\n",
    "except ValueError as e:\n",
    "    print(f\"❌ Configuration error: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Campaign creation failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f364022",
   "metadata": {},
   "source": [
    "### Creating Stations\n",
    "\n",
    "Once you have a campaign, you need to create stations within it. Stations represent specific monitoring locations where sensors collect data.\n",
    "\n",
    "#### Station Requirements\n",
    "\n",
    "**Required Fields:**\n",
    "- `campaign_id`: ID of the parent campaign (must exist)\n",
    "- `name`: Unique name for the monitoring station\n",
    "- `description`: Details about the station location and purpose\n",
    "- `latitude`: Decimal degrees (e.g., 30.2672)\n",
    "- `longitude`: Decimal degrees (e.g., -97.7431)\n",
    "\n",
    "#### Station Best Practices\n",
    "\n",
    "📍 **Location Data:**\n",
    "- Ensure coordinates are in decimal degrees format\n",
    "- Use WGS84 coordinate system (standard GPS coordinates)\n",
    "- Verify coordinates are accurate for your monitoring location\n",
    "- Test coordinates in mapping software before creating stations\n",
    "\n",
    "🏷️ **Station Naming:**\n",
    "- Use descriptive names that indicate location or purpose\n",
    "- Include geographic references or landmarks\n",
    "- Examples: \"River Bridge Station\", \"Industrial District Monitor\"\n",
    "\n",
    "📝 **Station Descriptions:**\n",
    "- Describe the physical location and surroundings\n",
    "- Note any special characteristics or constraints\n",
    "- Include installation details or access information\n",
    "\n",
    "#### Alternative: Web Interface for Stations\n",
    "\n",
    "If you prefer using the web interface:\n",
    "\n",
    "1. **Navigate to Campaign:**\n",
    "   - Go to your created campaign in the web portal\n",
    "   - Access the campaign details page\n",
    "\n",
    "2. **Create Station:**\n",
    "   - Go to the \"Stations\" section within the campaign\n",
    "   - Click \"Add Station\"\n",
    "   - Provide station details and coordinates\n",
    "   - Save to get your Station ID\n",
    "\n",
    "3. **Note the Station ID:**\n",
    "   - Copy the Station ID for use in data uploads\n",
    "\n",
    "\n",
    "💡 **Pro Tip:** Save your campaign and station IDs in a configuration file or notebook cell for easy reuse across multiple data uploads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8ee3af6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_station(\n",
    "    station_data: Dict[str, Any],\n",
    "    campaign_id: int,\n",
    "    token: str,\n",
    "    base_url: str = \"https://upstream-dso.tacc.utexas.edu/dev\"\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Create a new station within a campaign.\n",
    "    \n",
    "    Args:\n",
    "        station_data: Dictionary containing station information\n",
    "        campaign_id: ID of the parent campaign\n",
    "        token: Authentication token\n",
    "        base_url: Base URL for the API\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing the created station data with ID\n",
    "    \"\"\"\n",
    "    url = f\"{base_url}/api/v1/campaigns/{campaign_id}/stations\"\n",
    "    \n",
    "    response = make_authenticated_request(\n",
    "        method=\"POST\",\n",
    "        url=url,\n",
    "        token=token,\n",
    "        json=station_data\n",
    "    )\n",
    "    \n",
    "    result = response.json()\n",
    "    print(f\"✅ Station created successfully!\")\n",
    "    print(f\"Station ID: {result.get('id')}\")\n",
    "    print(f\"Station Name: {station_data.get('name')}\")\n",
    "    print(f\"Project ID: {station_data.get('projectid')}\")\n",
    "    print(f\"Contact: {station_data.get('contact_name')}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "def load_station_config(config_path: str = \"stations/station.json\") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Load station configuration from JSON file.\n",
    "    \n",
    "    Args:\n",
    "        config_path: Path to the station configuration JSON file\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing station configuration data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(config_path, 'r', encoding='utf-8') as file:\n",
    "            config = json.load(file)\n",
    "            print(f\"📄 Loaded station config from: {config_path}\")\n",
    "            return config\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"Station config file not found: {config_path}\")\n",
    "    except json.JSONDecodeError as e:\n",
    "        raise ValueError(f\"Invalid JSON in station config file: {e}\")\n",
    "\n",
    "def load_and_create_station(\n",
    "    campaign_id: int,\n",
    "    config_path: str = \"stations/station.json\",\n",
    "    token: str = None,\n",
    "    base_url: str = \"https://upstream-dso.tacc.utexas.edu/dev\"\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Load station configuration from JSON and create the station.\n",
    "    \n",
    "    Args:\n",
    "        campaign_id: ID of the parent campaign\n",
    "        config_path: Path to the station configuration JSON file\n",
    "        token: Authentication token\n",
    "        base_url: Base URL for the API\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing the created station data with ID\n",
    "    \"\"\"\n",
    "    # Load configuration\n",
    "    station_config = load_station_config(config_path)\n",
    "    \n",
    "    # Validate required fields\n",
    "    required_fields = [\"name\", \"projectid\", \"description\", \"contact_name\", \"contact_email\", \"active\", \"start_date\"]\n",
    "    for field in required_fields:\n",
    "        if field not in station_config:\n",
    "            raise ValueError(f\"Missing required field '{field}' in station config\")\n",
    "    \n",
    "    # Display configuration summary\n",
    "    print(f\"📋 Station Configuration Summary:\")\n",
    "    print(f\"  Name: {station_config['name']}\")\n",
    "    print(f\"  Project ID: {station_config['projectid']}\")\n",
    "    print(f\"  Description: {station_config['description'][:100]}...\")\n",
    "    print(f\"  Contact: {station_config['contact_name']}\")\n",
    "    print(f\"  Active: {station_config['active']}\")\n",
    "    print(f\"  Start Date: {station_config['start_date']}\")\n",
    "    \n",
    "    # Create the station\n",
    "    station = create_station(\n",
    "        station_data=station_config,\n",
    "        campaign_id=campaign_id,\n",
    "        token=token,\n",
    "        base_url=base_url\n",
    "    )\n",
    "    \n",
    "    return station\n",
    "\n",
    "def load_and_create_multiple_stations(\n",
    "    campaign_id: int,\n",
    "    config_path: str = \"stations/stations.json\",\n",
    "    token: str = None,\n",
    "    base_url: str = \"https://upstream-dso.tacc.utexas.edu/dev\"\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Load multiple station configurations from JSON and create all stations.\n",
    "    \n",
    "    Args:\n",
    "        campaign_id: ID of the parent campaign\n",
    "        config_path: Path to the stations configuration JSON file\n",
    "        token: Authentication token\n",
    "        base_url: Base URL for the API\n",
    "        \n",
    "    Returns:\n",
    "        List of dictionaries containing the created station data\n",
    "    \"\"\"\n",
    "    # Load configuration\n",
    "    with open(config_path, 'r', encoding='utf-8') as file:\n",
    "        stations_config = json.load(file)\n",
    "    \n",
    "    created_stations = []\n",
    "    \n",
    "    # Handle both single station and multiple stations format\n",
    "    if \"stations\" in stations_config:\n",
    "        station_list = stations_config[\"stations\"]\n",
    "    else:\n",
    "        station_list = [stations_config]  # Single station format\n",
    "    \n",
    "    print(f\"📋 Creating {len(station_list)} station(s)...\")\n",
    "    \n",
    "    for i, station_config in enumerate(station_list, 1):\n",
    "        print(f\"\\n--- Creating Station {i}/{len(station_list)} ---\")\n",
    "        \n",
    "        try:\n",
    "            station = create_station(\n",
    "                station_data=station_config,\n",
    "                campaign_id=campaign_id,\n",
    "                token=token,\n",
    "                base_url=base_url\n",
    "            )\n",
    "            created_stations.append(station)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to create station '{station_config.get('name', 'Unknown')}': {e}\")\n",
    "            continue\n",
    "    \n",
    "    return created_stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e3f0d1ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Creating Single Station from Configuration ===\n",
      "📄 Loaded station config from: stations/station.json\n",
      "📋 Station Configuration Summary:\n",
      "  Name: Cow Bayou near Mauriceville\n",
      "  Project ID: SETx-UIFL Beaumont\n",
      "  Description: Beaumont Run stream gauge at Cow Bayou...\n",
      "  Contact: Nick Brake\n",
      "  Active: True\n",
      "  Start Date: 2025-06-02T14:42:00+0000\n",
      "✅ Station created successfully!\n",
      "Station ID: 14\n",
      "Station Name: Cow Bayou near Mauriceville\n",
      "Project ID: SETx-UIFL Beaumont\n",
      "Contact: Nick Brake\n",
      "\n",
      "🎉 Station setup complete!\n",
      "Station ID: 14\n",
      "\n",
      "==================================================\n",
      "=== Creating Multiple Stations from Configuration ===\n",
      "❌ Configuration file error: [Errno 2] No such file or directory: 'stations/stations.json'\n",
      "💡 Please create a stations/stations.json file with your station details\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Creating Single Station from Configuration ===\")\n",
    "try:\n",
    "    station = load_and_create_station(\n",
    "        campaign_id=campaign_id,\n",
    "        config_path=\"stations/station.json\",\n",
    "        token=token\n",
    "    )\n",
    "    \n",
    "    station_id = station['id']\n",
    "    print(f\"\\n🎉 Station setup complete!\")\n",
    "    print(f\"Station ID: {station_id}\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"❌ Configuration file error: {e}\")\n",
    "    print(\"💡 Please create a stations/station.json file with your station details\")\n",
    "except ValueError as e:\n",
    "    print(f\"❌ Configuration error: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Station creation failed: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"=== Creating Multiple Stations from Configuration ===\")\n",
    "try:\n",
    "    stations = load_and_create_multiple_stations(\n",
    "        campaign_id=campaign_id,\n",
    "        config_path=\"stations/stations.json\",\n",
    "        token=token\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n🎉 Created {len(stations)} station(s) successfully!\")\n",
    "    for station in stations:\n",
    "        print(f\"  • {station.get('name', 'Unknown')} (ID: {station['id']})\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"❌ Configuration file error: {e}\")\n",
    "    print(\"💡 Please create a stations/stations.json file with your station details\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Multiple stations creation failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593ba293",
   "metadata": {},
   "source": [
    "## CSV Data Upload Function Documentation\n",
    "\n",
    "### Overview\n",
    "\n",
    "The `upload_csv_data` function provides a streamlined way to upload sensor and measurement data to the Upstream platform via CSV files. This function handles file validation, authentication, and provides detailed feedback on the upload process.\n",
    "\n",
    "\n",
    "### Parameters\n",
    "\n",
    "| Parameter | Type | Required | Description |\n",
    "|-----------|------|----------|-------------|\n",
    "| `campaign_id` | `int` | ✅ | Unique identifier for the target campaign |\n",
    "| `station_id` | `int` | ✅ | Unique identifier for the target station within the campaign |\n",
    "| `sensors_file_path` | `str` | ✅ | Local file path to the sensors CSV file |\n",
    "| `measurements_file_path` | `str` | ✅ | Local file path to the measurements CSV file |\n",
    "| `token` | `str` | ✅ | Authentication token for API access |\n",
    "| `base_url` | `str` | ❌ | Base URL for the Upstream API (defaults to dev environment) |\n",
    "\n",
    "### Return Value\n",
    "\n",
    "Returns a `Dict[str, Any]` containing the upload response data with statistics including:\n",
    "- Total sensors processed\n",
    "- Total measurements added to database\n",
    "- Data processing time\n",
    "\n",
    "### Features\n",
    "\n",
    "#### 🔍 **File Validation**\n",
    "- Automatically checks if both CSV files exist before attempting upload\n",
    "- Raises `FileNotFoundError` with descriptive messages for missing files\n",
    "\n",
    "#### 📊 **Progress Tracking**\n",
    "- Displays upload parameters for verification\n",
    "- Shows real-time upload status with emoji indicators\n",
    "- Provides detailed statistics upon completion\n",
    "\n",
    "#### 🔐 **Secure Upload**\n",
    "- Uses authenticated requests via the `make_authenticated_request` helper\n",
    "- Properly formats files for multipart form data upload\n",
    "\n",
    "#### 🎯 **Error Handling**\n",
    "- Pre-upload file existence validation\n",
    "- Clear error messages for troubleshooting\n",
    "\n",
    "### API Endpoint\n",
    "\n",
    "The function uploads to the following endpoint:\n",
    "```\n",
    "POST {base_url}/api/v1/uploadfile_csv/campaign/{campaign_id}/station/{station_id}/sensor\n",
    "```\n",
    "\n",
    "### File Format Requirements\n",
    "\n",
    "#### Sensors CSV\n",
    "- Must contain sensor definition data\n",
    "- Uploaded as `upload_file_sensors` form field\n",
    "\n",
    "#### Measurements CSV  \n",
    "- Must contain measurement data corresponding to the sensors\n",
    "- Uploaded as `upload_file_measurements` form field\n",
    "\n",
    "### Console Output Example\n",
    "\n",
    "```\n",
    "=== Uploading CSV Data ===\n",
    "Campaign ID: 123\n",
    "Station ID: 456\n",
    "Sensors file: ./data/sensors.csv\n",
    "Measurements file: ./data/measurements.csv\n",
    "📤 Uploading files...\n",
    "✅ Upload completed successfully!\n",
    "📊 Upload Statistics:\n",
    "  • Sensors processed: 15\n",
    "  • Measurements added: 1,250\n",
    "  • Processing time: 2.3s\n",
    "```\n",
    "\n",
    "### Dependencies\n",
    "\n",
    "- `os` - For file existence checking\n",
    "- `make_authenticated_request` - Custom function for authenticated API calls\n",
    "- `Dict`, `Any` from `typing` - For type hints\n",
    "\n",
    "### Error Scenarios\n",
    "\n",
    "| Error Type | Cause | Solution |\n",
    "|------------|-------|----------|\n",
    "| `FileNotFoundError` | CSV file doesn't exist at specified path | Verify file paths are correct |\n",
    "| Authentication errors | Invalid or expired token | Refresh authentication token |\n",
    "| API errors | Server issues or invalid parameters | Check campaign/station IDs and API status |\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Validate Data First**: Ensure your CSV files are properly formatted before upload\n",
    "2. **Check Permissions**: Verify you have write access to the specified campaign/station\n",
    "3. **Monitor Output**: Pay attention to the upload statistics to confirm expected data volumes\n",
    "4. **Handle Errors**: Always wrap calls in try-catch blocks for production use\n",
    "5. **Use Absolute Paths**: Prefer absolute file paths to avoid path resolution issues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bee3ba22-8018-4d8d-86ed-04fb62ebc6b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Listing Available Data Files ===\n",
      "📁 Files found in ./data/:\n",
      "  • Total CSV files: 2\n",
      "  • Sensor files: 1\n",
      "  • Measurement files: 1\n",
      "📄 All CSV files:\n",
      "    - measurements.csv (906,949 bytes)\n",
      "    - sensors.csv (173 bytes)\n",
      "\n",
      "==================================================\n",
      "=== Uploading CSV Data (Standard Method) ===\n",
      "=== Uploading CSV Data ===\n",
      "Campaign ID: 10\n",
      "Station ID: 14\n",
      "Data Directory: ./data/\n",
      "Sensors file: ./data/sensors.csv\n",
      "Measurements file: ./data/measurements.csv\n",
      "📁 File Information:\n",
      "  • Sensors file size: 173 bytes\n",
      "  • Measurements file size: 906,949 bytes\n",
      "📤 Uploading files...\n",
      "✅ Upload completed successfully!\n",
      "📊 Upload Statistics:\n",
      "  • Sensors processed: 3\n",
      "  • Measurements added: 26881\n",
      "  • Processing time: 9.4 seconds.\n",
      "\n",
      "🎉 Data upload complete!\n",
      "\n",
      "==================================================\n",
      "=== Uploading CSV Data (Auto-Detection Method) ===\n",
      "=== Auto-detecting Data Files ===\n",
      "📁 Files found in ./data/:\n",
      "  • Total CSV files: 2\n",
      "  • Sensor files: 1\n",
      "  • Measurement files: 1\n",
      "📄 All CSV files:\n",
      "    - measurements.csv (906,949 bytes)\n",
      "    - sensors.csv (173 bytes)\n",
      "=== Uploading CSV Data ===\n",
      "Campaign ID: 10\n",
      "Station ID: 14\n",
      "Data Directory: ./data/\n",
      "Sensors file: ./data/sensors.csv\n",
      "Measurements file: ./data/measurements.csv\n",
      "📁 File Information:\n",
      "  • Sensors file size: 173 bytes\n",
      "  • Measurements file size: 906,949 bytes\n",
      "📤 Uploading files...\n",
      "✅ Upload completed successfully!\n",
      "📊 Upload Statistics:\n",
      "  • Sensors processed: 3\n",
      "  • Measurements added: 0\n",
      "  • Processing time: 8.2 seconds.\n",
      "\n",
      "🎉 Auto-detected data upload complete!\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def upload_csv_data(\n",
    "    campaign_id: int,\n",
    "    station_id: int,\n",
    "    token: str,\n",
    "    data_dir: str = \"./data/\",\n",
    "    sensors_filename: str = \"sensors.csv\",\n",
    "    measurements_filename: str = \"measurements.csv\",\n",
    "    base_url: str = \"https://upstream-dso.tacc.utexas.edu/dev\"\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Upload sensor and measurement CSV files to Upstream from data directory.\n",
    "    \n",
    "    Args:\n",
    "        campaign_id: ID of the target campaign\n",
    "        station_id: ID of the target station\n",
    "        token: Access token\n",
    "        data_dir: Directory containing CSV files (default: \"./data/\")\n",
    "        sensors_filename: Name of sensors CSV file (default: \"sensors.csv\")\n",
    "        measurements_filename: Name of measurements CSV file (default: \"measurements.csv\")\n",
    "        base_url: Base URL for the API\n",
    "        \n",
    "    Returns:\n",
    "        Upload response data\n",
    "    \"\"\"\n",
    "    # Construct file paths\n",
    "    sensors_file_path = os.path.join(data_dir, sensors_filename)\n",
    "    measurements_file_path = os.path.join(data_dir, measurements_filename)\n",
    "    \n",
    "    upload_url = f\"{base_url}/api/v1/uploadfile_csv/campaign/{campaign_id}/station/{station_id}/sensor\"\n",
    "    \n",
    "    print(f\"=== Uploading CSV Data ===\")\n",
    "    print(f\"Campaign ID: {campaign_id}\")\n",
    "    print(f\"Station ID: {station_id}\")\n",
    "    print(f\"Data Directory: {data_dir}\")\n",
    "    print(f\"Sensors file: {sensors_file_path}\")\n",
    "    print(f\"Measurements file: {measurements_file_path}\")\n",
    "    \n",
    "    # Verify files exist\n",
    "    if not os.path.exists(sensors_file_path):\n",
    "        raise FileNotFoundError(f\"Sensors file not found: {sensors_file_path}\")\n",
    "    if not os.path.exists(measurements_file_path):\n",
    "        raise FileNotFoundError(f\"Measurements file not found: {measurements_file_path}\")\n",
    "    \n",
    "    # Display file information\n",
    "    sensors_size = os.path.getsize(sensors_file_path)\n",
    "    measurements_size = os.path.getsize(measurements_file_path)\n",
    "    print(f\"📁 File Information:\")\n",
    "    print(f\"  • Sensors file size: {sensors_size:,} bytes\")\n",
    "    print(f\"  • Measurements file size: {measurements_size:,} bytes\")\n",
    "    \n",
    "    # Prepare files for upload\n",
    "    with open(sensors_file_path, 'rb') as sensors_file, \\\n",
    "         open(measurements_file_path, 'rb') as measurements_file:\n",
    "        \n",
    "        files = {\n",
    "            'upload_file_sensors': (sensors_filename, sensors_file, 'text/csv'),\n",
    "            'upload_file_measurements': (measurements_filename, measurements_file, 'text/csv')\n",
    "        }\n",
    "        \n",
    "        print(\"📤 Uploading files...\")\n",
    "        response = make_authenticated_request(\n",
    "            method=\"POST\",\n",
    "            url=upload_url,\n",
    "            token=token,\n",
    "            files=files\n",
    "        )\n",
    "        \n",
    "        result = response.json()\n",
    "        print(\"✅ Upload completed successfully!\")\n",
    "        \n",
    "        # Display upload statistics\n",
    "        print(f\"📊 Upload Statistics:\")\n",
    "        print(f\"  • Sensors processed: {result.get('Total sensors processed', 'N/A')}\")\n",
    "        print(f\"  • Measurements added: {result.get('Total measurements added to database', 'N/A')}\")\n",
    "        print(f\"  • Processing time: {result.get('Data Processing time', 'N/A')}\")\n",
    "        \n",
    "        return result\n",
    "\n",
    "def list_data_files(data_dir: str = \"./data/\") -> Dict[str, list]:\n",
    "    \"\"\"\n",
    "    List all CSV files in the data directory.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Directory to search for CSV files\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with lists of found files\n",
    "    \"\"\"\n",
    "    if not os.path.exists(data_dir):\n",
    "        print(f\"❌ Data directory not found: {data_dir}\")\n",
    "        return {\"csv_files\": [], \"sensors_files\": [], \"measurements_files\": []}\n",
    "    \n",
    "    # Find all CSV files\n",
    "    csv_pattern = os.path.join(data_dir, \"*.csv\")\n",
    "    csv_files = glob.glob(csv_pattern)\n",
    "    \n",
    "    # Categorize files\n",
    "    sensors_files = [f for f in csv_files if 'sensor' in os.path.basename(f).lower()]\n",
    "    measurements_files = [f for f in csv_files if 'measurement' in os.path.basename(f).lower()]\n",
    "    \n",
    "    print(f\"📁 Files found in {data_dir}:\")\n",
    "    print(f\"  • Total CSV files: {len(csv_files)}\")\n",
    "    print(f\"  • Sensor files: {len(sensors_files)}\")\n",
    "    print(f\"  • Measurement files: {len(measurements_files)}\")\n",
    "    \n",
    "    if csv_files:\n",
    "        print(f\"📄 All CSV files:\")\n",
    "        for file in csv_files:\n",
    "            size = os.path.getsize(file)\n",
    "            print(f\"    - {os.path.basename(file)} ({size:,} bytes)\")\n",
    "    \n",
    "    return {\n",
    "        \"csv_files\": csv_files,\n",
    "        \"sensors_files\": sensors_files,\n",
    "        \"measurements_files\": measurements_files\n",
    "    }\n",
    "\n",
    "def upload_data_with_auto_detection(\n",
    "    campaign_id: int,\n",
    "    station_id: int,\n",
    "    token: str,\n",
    "    data_dir: str = \"./data/\",\n",
    "    base_url: str = \"https://upstream-dso.tacc.utexas.edu/dev\"\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Upload CSV data with automatic file detection.\n",
    "    \n",
    "    Args:\n",
    "        campaign_id: ID of the target campaign\n",
    "        station_id: ID of the target station\n",
    "        token: Access token\n",
    "        data_dir: Directory containing CSV files\n",
    "        base_url: Base URL for the API\n",
    "        \n",
    "    Returns:\n",
    "        Upload response data\n",
    "    \"\"\"\n",
    "    print(\"=== Auto-detecting Data Files ===\")\n",
    "    files_info = list_data_files(data_dir)\n",
    "    \n",
    "    # Try to find sensors and measurements files\n",
    "    sensors_file = None\n",
    "    measurements_file = None\n",
    "    \n",
    "    # Look for standard filenames first\n",
    "    standard_sensors = os.path.join(data_dir, \"sensors.csv\")\n",
    "    standard_measurements = os.path.join(data_dir, \"measurements.csv\")\n",
    "    \n",
    "    if os.path.exists(standard_sensors):\n",
    "        sensors_file = \"sensors.csv\"\n",
    "    elif files_info[\"sensors_files\"]:\n",
    "        sensors_file = os.path.basename(files_info[\"sensors_files\"][0])\n",
    "        print(f\"🔍 Using detected sensors file: {sensors_file}\")\n",
    "    \n",
    "    if os.path.exists(standard_measurements):\n",
    "        measurements_file = \"measurements.csv\"\n",
    "    elif files_info[\"measurements_files\"]:\n",
    "        measurements_file = os.path.basename(files_info[\"measurements_files\"][0])\n",
    "        print(f\"🔍 Using detected measurements file: {measurements_file}\")\n",
    "    \n",
    "    if not sensors_file or not measurements_file:\n",
    "        raise FileNotFoundError(\n",
    "            f\"Could not find required files. \"\n",
    "            f\"Sensors: {sensors_file}, Measurements: {measurements_file}\"\n",
    "        )\n",
    "    \n",
    "    # Upload the files\n",
    "    return upload_csv_data(\n",
    "        campaign_id=campaign_id,\n",
    "        station_id=station_id,\n",
    "        token=token,\n",
    "        data_dir=data_dir,\n",
    "        sensors_filename=sensors_file,\n",
    "        measurements_filename=measurements_file,\n",
    "        base_url=base_url\n",
    "    )\n",
    "\n",
    "# Usage examples\n",
    "print(\"=== Listing Available Data Files ===\")\n",
    "data_files = list_data_files(\"./data/\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"=== Uploading CSV Data (Standard Method) ===\")\n",
    "try:\n",
    "    # Upload using standard filenames\n",
    "    result = upload_csv_data(\n",
    "        campaign_id=campaign_id,\n",
    "        station_id=station_id,\n",
    "        token=token,\n",
    "        data_dir=\"./data/\",\n",
    "        sensors_filename=\"sensors.csv\",\n",
    "        measurements_filename=\"measurements.csv\"\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n🎉 Data upload complete!\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"❌ File error: {e}\")\n",
    "    print(\"💡 Make sure your CSV files are in the ./data/ directory\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Upload failed: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"=== Uploading CSV Data (Auto-Detection Method) ===\")\n",
    "try:\n",
    "    # Upload with automatic file detection\n",
    "    result = upload_data_with_auto_detection(\n",
    "        campaign_id=campaign_id,\n",
    "        station_id=station_id,\n",
    "        token=token,\n",
    "        data_dir=\"./data/\"\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n🎉 Auto-detected data upload complete!\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"❌ File detection error: {e}\")\n",
    "    print(\"💡 Make sure your CSV files are in the ./data/ directory with 'sensor' and 'measurement' in their names\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Auto-upload failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c765ce",
   "metadata": {},
   "source": [
    "### CSV File Format Examples\n",
    "\n",
    "#### Sensors CSV Format\n",
    "\n",
    "Your `sensors.csv` file defines the sensor metadata and should follow this structure:\n",
    "\n",
    "```csv\n",
    "alias,variablename,units,postprocess,postprocessscript\n",
    "temp_sensor_01,Air Temperature,°C,,\n",
    "humidity_01,Relative Humidity,%,,\n",
    "pressure_01,Atmospheric Pressure,hPa,,\n",
    "wind_speed_01,Wind Speed,m/s,true,wind_correction_script\n",
    "```\n",
    "\n",
    "**Column Descriptions:**\n",
    "- `alias`: Unique identifier for the sensor (used as column header in measurements)\n",
    "- `variablename`: Human-readable description of what the sensor measures\n",
    "- `units`: Measurement units (e.g., °C, %, hPa, m/s)\n",
    "- `postprocess`: Boolean flag indicating if post-processing is required\n",
    "- `postprocessscript`: Name of the post-processing script (if applicable)\n",
    "\n",
    "#### Measurements CSV Format\n",
    "\n",
    "Your `measurements.csv` file contains the actual sensor data and should follow this structure:\n",
    "\n",
    "```csv\n",
    "collectiontime,Lat_deg,Lon_deg,temp_sensor_01,humidity_01,pressure_01,wind_speed_01\n",
    "2024-01-15T10:30:00,30.2672,-97.7431,23.5,65.2,1013.25,2.3\n",
    "2024-01-15T10:31:00,30.2673,-97.7432,23.7,64.8,1013.20,2.1\n",
    "2024-01-15T10:32:00,30.2674,-97.7433,23.9,64.5,1013.15,1.8\n",
    "2024-01-15T10:33:00,30.2675,-97.7434,,64.2,1013.10,1.9\n",
    "```\n",
    "\n",
    "**Required Columns:**\n",
    "- `collectiontime`: Timestamp in ISO 8601 format (YYYY-MM-DDTHH:MM:SS)\n",
    "- `Lat_deg`: Latitude in decimal degrees\n",
    "- `Lon_deg`: Longitude in decimal degrees\n",
    "\n",
    "**Sensor Data Columns:**\n",
    "- Each sensor `alias` from sensors.csv becomes a column header\n",
    "- Column names must exactly match the sensor aliases\n",
    "- Empty values are automatically handled (see row 4 in example)\n",
    "\n",
    "#### Important File Format Notes\n",
    "\n",
    "⚠️ **Critical Requirements:**\n",
    "- Each sensor `alias` from sensors.csv becomes a column in measurements.csv\n",
    "- `collectiontime`, `Lat_deg`, and `Lon_deg` are required columns in measurements.csv\n",
    "- Empty values are handled automatically by the system\n",
    "- Maximum file size is **500 MB per file**\n",
    "- Use UTF-8 encoding for both files\n",
    "- Timestamps should be in UTC or include timezone information\n",
    "\n",
    "📝 **Best Practices:**\n",
    "- Keep sensor aliases short but descriptive\n",
    "- Use consistent naming conventions (e.g., `sensor_type_number`)\n",
    "- Ensure measurement values match the units specified in sensors.csv\n",
    "- Include all sensors in measurements.csv even if some readings are missing\n",
    "\n",
    "\n",
    "#### Helper Function Features\n",
    "\n",
    "🔍 **Campaign Discovery:**\n",
    "- List all campaigns you have access to\n",
    "- View campaign metadata and descriptions\n",
    "- Identify the correct campaign ID for your data\n",
    "\n",
    "🏗️ **Station Management:**\n",
    "- List all stations within a campaign\n",
    "- View station details and locations\n",
    "- Find the appropriate station ID for your sensors\n",
    "\n",
    "💡 **Integration Tip:**\n",
    "Use these helper functions before uploading data to ensure you're targeting the correct campaign and station IDs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "062450c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import tempfile\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Optional\n",
    "import time\n",
    "import math\n",
    "\n",
    "def get_file_info(file_path: str) -> Dict[str, Any]:\n",
    "    \"\"\"Get detailed information about a CSV file.\"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        return {}\n",
    "    \n",
    "    file_size = os.path.getsize(file_path)\n",
    "    \n",
    "    # Count rows efficiently\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        row_count = sum(1 for line in f) - 1  # Subtract header row\n",
    "    \n",
    "    return {\n",
    "        'size_bytes': file_size,\n",
    "        'size_mb': file_size / (1024 * 1024),\n",
    "        'row_count': row_count,\n",
    "        'estimated_chunk_count': lambda chunk_size: math.ceil(row_count / chunk_size)\n",
    "    }\n",
    "\n",
    "def create_csv_chunks(\n",
    "    file_path: str,\n",
    "    chunk_size: int = 10000,\n",
    "    output_dir: Optional[str] = None,\n",
    "    max_file_size_mb: int = 50\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Split a large CSV file into smaller chunks.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the large CSV file\n",
    "        chunk_size: Number of rows per chunk\n",
    "        output_dir: Directory to store chunk files (temp dir if None)\n",
    "        max_file_size_mb: Maximum file size per chunk in MB\n",
    "        \n",
    "    Returns:\n",
    "        List of chunk file paths\n",
    "    \"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    # Create output directory\n",
    "    if output_dir is None:\n",
    "        output_dir = tempfile.mkdtemp(prefix=\"csv_chunks_\")\n",
    "    else:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    file_info = get_file_info(file_path)\n",
    "    filename = os.path.basename(file_path)\n",
    "    name, ext = os.path.splitext(filename)\n",
    "    \n",
    "    print(f\"📦 Chunking {filename}:\")\n",
    "    print(f\"  • Total rows: {file_info['row_count']:,}\")\n",
    "    print(f\"  • File size: {file_info['size_mb']:.2f} MB\")\n",
    "    print(f\"  • Chunk size: {chunk_size:,} rows\")\n",
    "    print(f\"  • Estimated chunks: {file_info['estimated_chunk_count'](chunk_size)}\")\n",
    "    \n",
    "    chunk_files = []\n",
    "    \n",
    "    try:\n",
    "        # Read and chunk the CSV file\n",
    "        chunk_num = 0\n",
    "        for chunk_df in pd.read_csv(file_path, chunksize=chunk_size):\n",
    "            chunk_num += 1\n",
    "            chunk_filename = f\"{name}_chunk_{chunk_num:03d}{ext}\"\n",
    "            chunk_path = os.path.join(output_dir, chunk_filename)\n",
    "            \n",
    "            # Save chunk\n",
    "            chunk_df.to_csv(chunk_path, index=False)\n",
    "            \n",
    "            # Check file size\n",
    "            chunk_size_mb = os.path.getsize(chunk_path) / (1024 * 1024)\n",
    "            if chunk_size_mb > max_file_size_mb:\n",
    "                print(f\"⚠️  Warning: Chunk {chunk_num} is {chunk_size_mb:.2f} MB (exceeds {max_file_size_mb} MB limit)\")\n",
    "            \n",
    "            chunk_files.append(chunk_path)\n",
    "            print(f\"  ✓ Created chunk {chunk_num}: {len(chunk_df)} rows, {chunk_size_mb:.2f} MB\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Clean up on error\n",
    "        for chunk_file in chunk_files:\n",
    "            if os.path.exists(chunk_file):\n",
    "                os.remove(chunk_file)\n",
    "        raise e\n",
    "    \n",
    "    print(f\"📦 Created {len(chunk_files)} chunks in {output_dir}\")\n",
    "    return chunk_files\n",
    "\n",
    "def upload_csv_data_chunked(\n",
    "    campaign_id: int,\n",
    "    station_id: int,\n",
    "    token: str,\n",
    "    data_dir: str = \"./data/\",\n",
    "    sensors_filename: str = \"sensors.csv\",\n",
    "    measurements_filename: str = \"measurements.csv\",\n",
    "    chunk_size: int = 10000,\n",
    "    max_file_size_mb: int = 50,\n",
    "    cleanup_chunks: bool = True,\n",
    "    base_url: str = \"https://upstream-dso.tacc.utexas.edu/dev\"\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Upload large CSV files using chunking strategy.\n",
    "    \n",
    "    Args:\n",
    "        campaign_id: ID of the target campaign\n",
    "        station_id: ID of the target station\n",
    "        token: Access token\n",
    "        data_dir: Directory containing CSV files\n",
    "        sensors_filename: Name of sensors CSV file\n",
    "        measurements_filename: Name of measurements CSV file\n",
    "        chunk_size: Number of rows per chunk\n",
    "        max_file_size_mb: Maximum file size per chunk in MB\n",
    "        cleanup_chunks: Whether to delete chunk files after upload\n",
    "        base_url: Base URL for the API\n",
    "        \n",
    "    Returns:\n",
    "        Aggregated upload response data\n",
    "    \"\"\"\n",
    "    print(f\"=== Chunked CSV Data Upload ===\")\n",
    "    print(f\"Campaign ID: {campaign_id}\")\n",
    "    print(f\"Station ID: {station_id}\")\n",
    "    print(f\"Chunk size: {chunk_size:,} rows\")\n",
    "    print(f\"Max chunk file size: {max_file_size_mb} MB\")\n",
    "    \n",
    "    # Construct file paths\n",
    "    sensors_file_path = os.path.join(data_dir, sensors_filename)\n",
    "    measurements_file_path = os.path.join(data_dir, measurements_filename)\n",
    "    \n",
    "    # Verify files exist\n",
    "    if not os.path.exists(sensors_file_path):\n",
    "        raise FileNotFoundError(f\"Sensors file not found: {sensors_file_path}\")\n",
    "    if not os.path.exists(measurements_file_path):\n",
    "        raise FileNotFoundError(f\"Measurements file not found: {measurements_file_path}\")\n",
    "    \n",
    "    # Get file information\n",
    "    sensors_info = get_file_info(sensors_file_path)\n",
    "    measurements_info = get_file_info(measurements_file_path)\n",
    "    \n",
    "    print(f\"\\n📁 File Analysis:\")\n",
    "    print(f\"  • Sensors: {sensors_info['row_count']:,} rows, {sensors_info['size_mb']:.2f} MB\")\n",
    "    print(f\"  • Measurements: {measurements_info['row_count']:,} rows, {measurements_info['size_mb']:.2f} MB\")\n",
    "    \n",
    "    # Create temporary directory for chunks\n",
    "    chunk_dir = tempfile.mkdtemp(prefix=\"upload_chunks_\")\n",
    "    \n",
    "    try:\n",
    "        # Create chunks\n",
    "        print(\"\\n--- Chunking Sensors File ---\")\n",
    "        sensors_chunks = create_csv_chunks(\n",
    "            sensors_file_path, \n",
    "            chunk_size=chunk_size,\n",
    "            output_dir=os.path.join(chunk_dir, \"sensors\"),\n",
    "            max_file_size_mb=max_file_size_mb\n",
    "        )\n",
    "        \n",
    "        print(\"\\n--- Chunking Measurements File ---\")\n",
    "        measurements_chunks = create_csv_chunks(\n",
    "            measurements_file_path,\n",
    "            chunk_size=chunk_size, \n",
    "            output_dir=os.path.join(chunk_dir, \"measurements\"),\n",
    "            max_file_size_mb=max_file_size_mb\n",
    "        )\n",
    "        \n",
    "        # Upload chunks\n",
    "        total_chunks = max(len(sensors_chunks), len(measurements_chunks))\n",
    "        successful_uploads = 0\n",
    "        failed_uploads = 0\n",
    "        aggregated_results = {\n",
    "            'total_sensors_processed': 0,\n",
    "            'total_measurements_added': 0,\n",
    "            'total_processing_time': 0,\n",
    "            'chunk_results': []\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n📤 Uploading {total_chunks} chunk pairs...\")\n",
    "        \n",
    "        for i in range(total_chunks):\n",
    "            chunk_num = i + 1\n",
    "            print(f\"\\n--- Uploading Chunk {chunk_num}/{total_chunks} ---\")\n",
    "            \n",
    "            try:\n",
    "                # Get chunk files (use last chunk if one file has fewer chunks)\n",
    "                sensors_chunk = sensors_chunks[min(i, len(sensors_chunks) - 1)]\n",
    "                measurements_chunk = measurements_chunks[min(i, len(measurements_chunks) - 1)]\n",
    "                \n",
    "                # Upload chunk pair\n",
    "                start_time = time.time()\n",
    "                \n",
    "                with open(sensors_chunk, 'rb') as sf, open(measurements_chunk, 'rb') as mf:\n",
    "                    files = {\n",
    "                        'upload_file_sensors': (os.path.basename(sensors_chunk), sf, 'text/csv'),\n",
    "                        'upload_file_measurements': (os.path.basename(measurements_chunk), mf, 'text/csv')\n",
    "                    }\n",
    "                    \n",
    "                    upload_url = f\"{base_url}/api/v1/uploadfile_csv/campaign/{campaign_id}/station/{station_id}/sensor\"\n",
    "                    response = make_authenticated_request(\n",
    "                        method=\"POST\",\n",
    "                        url=upload_url,\n",
    "                        token=token,\n",
    "                        files=files\n",
    "                    )\n",
    "                \n",
    "                upload_time = time.time() - start_time\n",
    "                result = response.json()\n",
    "                \n",
    "                # Aggregate results\n",
    "                aggregated_results['total_sensors_processed'] += result.get('Total sensors processed', 0)\n",
    "                aggregated_results['total_measurements_added'] += result.get('Total measurements added to database', 0)\n",
    "                aggregated_results['total_processing_time'] += upload_time\n",
    "                aggregated_results['chunk_results'].append({\n",
    "                    'chunk': chunk_num,\n",
    "                    'sensors_processed': result.get('Total sensors processed', 0),\n",
    "                    'measurements_added': result.get('Total measurements added to database', 0),\n",
    "                    'upload_time': upload_time\n",
    "                })\n",
    "                \n",
    "                successful_uploads += 1\n",
    "                print(f\"  ✅ Chunk {chunk_num} uploaded successfully\")\n",
    "                print(f\"     • Sensors: {result.get('Total sensors processed', 0)}\")\n",
    "                print(f\"     • Measurements: {result.get('Total measurements added to database', 0)}\")\n",
    "                print(f\"     • Time: {upload_time:.2f}s\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                failed_uploads += 1\n",
    "                print(f\"  ❌ Chunk {chunk_num} failed: {e}\")\n",
    "                aggregated_results['chunk_results'].append({\n",
    "                    'chunk': chunk_num,\n",
    "                    'error': str(e)\n",
    "                })\n",
    "        \n",
    "        # Final results\n",
    "        print(f\"\\n📊 Chunked Upload Summary:\")\n",
    "        print(f\"  • Total chunks: {total_chunks}\")\n",
    "        print(f\"  • Successful: {successful_uploads}\")\n",
    "        print(f\"  • Failed: {failed_uploads}\")\n",
    "        print(f\"  • Total sensors processed: {aggregated_results['total_sensors_processed']:,}\")\n",
    "        print(f\"  • Total measurements added: {aggregated_results['total_measurements_added']:,}\")\n",
    "        print(f\"  • Total processing time: {aggregated_results['total_processing_time']:.2f}s\")\n",
    "        \n",
    "        if failed_uploads > 0:\n",
    "            print(f\"⚠️  {failed_uploads} chunks failed to upload\")\n",
    "        \n",
    "        return aggregated_results\n",
    "        \n",
    "    finally:\n",
    "        # Cleanup chunks if requested\n",
    "        if cleanup_chunks and os.path.exists(chunk_dir):\n",
    "            print(f\"🧹 Cleaning up chunk files from {chunk_dir}\")\n",
    "            shutil.rmtree(chunk_dir)\n",
    "\n",
    "def list_data_files(data_dir: str = \"./data/\") -> Dict[str, list]:\n",
    "    \"\"\"List all CSV files in the data directory.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Directory to search for CSV files\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with lists of found files\n",
    "    \"\"\"\n",
    "    if not os.path.exists(data_dir):\n",
    "        print(f\"❌ Data directory not found: {data_dir}\")\n",
    "        return {\"csv_files\": [], \"sensors_files\": [], \"measurements_files\": []}\n",
    "    \n",
    "    # Find all CSV files\n",
    "    csv_pattern = os.path.join(data_dir, \"*.csv\")\n",
    "    csv_files = glob.glob(csv_pattern)\n",
    "    \n",
    "    # Categorize files\n",
    "    sensors_files = [f for f in csv_files if 'sensor' in os.path.basename(f).lower()]\n",
    "    measurements_files = [f for f in csv_files if 'measurement' in os.path.basename(f).lower()]\n",
    "    \n",
    "    print(f\"📁 Files found in {data_dir}:\")\n",
    "    print(f\"  • Total CSV files: {len(csv_files)}\")\n",
    "    print(f\"  • Sensor files: {len(sensors_files)}\")\n",
    "    print(f\"  • Measurement files: {len(measurements_files)}\")\n",
    "    \n",
    "    if csv_files:\n",
    "        print(f\"📄 All CSV files:\")\n",
    "        for file in csv_files:\n",
    "            size = os.path.getsize(file)\n",
    "            print(f\"    - {os.path.basename(file)} ({size:,} bytes)\")\n",
    "    \n",
    "    return {\n",
    "        \"csv_files\": csv_files,\n",
    "        \"sensors_files\": sensors_files,\n",
    "        \"measurements_files\": measurements_files\n",
    "    }\n",
    "\n",
    "def upload_data_with_auto_detection(\n",
    "    campaign_id: int,\n",
    "    station_id: int,\n",
    "    token: str,\n",
    "    data_dir: str = \"./data/\",\n",
    "    use_chunking: bool = False,\n",
    "    chunk_size: int = 10000,\n",
    "    max_file_size_mb: int = 50,\n",
    "    base_url: str = \"https://upstream-dso.tacc.utexas.edu/dev\"\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Upload CSV data with automatic file detection.\n",
    "    \n",
    "    Args:\n",
    "        campaign_id: ID of the target campaign\n",
    "        station_id: ID of the target station\n",
    "        token: Access token\n",
    "        data_dir: Directory containing CSV files\n",
    "        use_chunking: Whether to use chunked upload\n",
    "        chunk_size: Number of rows per chunk (if chunking)\n",
    "        max_file_size_mb: Maximum file size per chunk in MB (if chunking)\n",
    "        base_url: Base URL for the API\n",
    "        \n",
    "    Returns:\n",
    "        Upload response data\n",
    "    \"\"\"\n",
    "    print(\"=== Auto-detecting Data Files ===\")\n",
    "    files_info = list_data_files(data_dir)\n",
    "    \n",
    "    # Try to find sensors and measurements files\n",
    "    sensors_file = None\n",
    "    measurements_file = None\n",
    "    \n",
    "    # Look for standard filenames first\n",
    "    standard_sensors = os.path.join(data_dir, \"sensors.csv\")\n",
    "    standard_measurements = os.path.join(data_dir, \"measurements.csv\")\n",
    "    \n",
    "    if os.path.exists(standard_sensors):\n",
    "        sensors_file = \"sensors.csv\"\n",
    "    elif files_info[\"sensors_files\"]:\n",
    "        sensors_file = os.path.basename(files_info[\"sensors_files\"][0])\n",
    "        print(f\"🔍 Using detected sensors file: {sensors_file}\")\n",
    "    \n",
    "    if os.path.exists(standard_measurements):\n",
    "        measurements_file = \"measurements.csv\"\n",
    "    elif files_info[\"measurements_files\"]:\n",
    "        measurements_file = os.path.basename(files_info[\"measurements_files\"][0])\n",
    "        print(f\"🔍 Using detected measurements file: {measurements_file}\")\n",
    "    \n",
    "    if not sensors_file or not measurements_file:\n",
    "        raise FileNotFoundError(\n",
    "            f\"Could not find required files. \"\n",
    "            f\"Sensors: {sensors_file}, Measurements: {measurements_file}\"\n",
    "        )\n",
    "    \n",
    "    # Upload the files using chunked method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe2d17b-abd9-4193-be54-2d43236e7f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_campaigns(token: str, base_url: str = \"https://upstream-dso.tacc.utexas.edu/dev\") -> Dict[str, Any]:\n",
    "    \"\"\"Get list of available campaigns.\"\"\"\n",
    "    url = f\"{base_url}/api/v1/campaigns\"\n",
    "    response = make_authenticated_request(\"GET\", url, token)\n",
    "    return response.json()\n",
    "\n",
    "def get_stations(campaign_id: int, token: str, base_url: str = \"https://upstream-dso.tacc.utexas.edu/dev\") -> Dict[str, Any]:\n",
    "    \"\"\"Get list of stations for a campaign.\"\"\"\n",
    "    url = f\"{base_url}/api/v1/campaigns/{campaign_id}/stations\"\n",
    "    response = make_authenticated_request(\"GET\", url, token)\n",
    "    return response.json()\n",
    "\n",
    "# Example: List available campaigns and stations\n",
    "\"\"\"\n",
    "print(\"=== Available Campaigns ===\")\n",
    "campaigns = get_campaigns(token)\n",
    "print(json.dumps(campaigns, indent=2))\n",
    "\n",
    "print(\"=== Available Stations ===\")\n",
    "stations = get_stations(CAMPAIGN_ID, token)\n",
    "print(json.dumps(stations, indent=2))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4e77a7f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Files found in ./data/:\n",
      "  • Total CSV files: 2\n",
      "  • Sensor files: 1\n",
      "  • Measurement files: 1\n",
      "📄 All CSV files:\n",
      "    - measurements.csv (906,949 bytes)\n",
      "    - sensors.csv (173 bytes)\n",
      "📈 Upload Progress Estimation:\n",
      "  • Total data: 0.87 MB, 8,964 rows\n",
      "  • Estimated upload time: 1.7 seconds\n",
      "=== Chunked CSV Data Upload ===\n",
      "Campaign ID: 10\n",
      "Station ID: 14\n",
      "Chunk size: 6,000 rows\n",
      "Max chunk file size: 30 MB\n",
      "\n",
      "📁 File Analysis:\n",
      "  • Sensors: 3 rows, 0.00 MB\n",
      "  • Measurements: 8,961 rows, 0.86 MB\n",
      "\n",
      "--- Chunking Sensors File ---\n",
      "📦 Chunking sensors.csv:\n",
      "  • Total rows: 3\n",
      "  • File size: 0.00 MB\n",
      "  • Chunk size: 6,000 rows\n",
      "  • Estimated chunks: 1\n",
      "  ✓ Created chunk 1: 3 rows, 0.00 MB\n",
      "📦 Created 1 chunks in /var/folders/ps/dx2yrk_1117grf32kqlw9qyh0000gq/T/upload_chunks_6ccx_w6c/sensors\n",
      "\n",
      "--- Chunking Measurements File ---\n",
      "📦 Chunking measurements.csv:\n",
      "  • Total rows: 8,961\n",
      "  • File size: 0.86 MB\n",
      "  • Chunk size: 6,000 rows\n",
      "  • Estimated chunks: 2\n",
      "  ✓ Created chunk 1: 6000 rows, 0.58 MB\n",
      "  ✓ Created chunk 2: 2961 rows, 0.29 MB\n",
      "📦 Created 2 chunks in /var/folders/ps/dx2yrk_1117grf32kqlw9qyh0000gq/T/upload_chunks_6ccx_w6c/measurements\n",
      "\n",
      "📤 Uploading 2 chunk pairs...\n",
      "\n",
      "--- Uploading Chunk 1/2 ---\n",
      "  ✅ Chunk 1 uploaded successfully\n",
      "     • Sensors: 3\n",
      "     • Measurements: 0\n",
      "     • Time: 5.75s\n",
      "\n",
      "--- Uploading Chunk 2/2 ---\n",
      "  ✅ Chunk 2 uploaded successfully\n",
      "     • Sensors: 3\n",
      "     • Measurements: 0\n",
      "     • Time: 2.87s\n",
      "\n",
      "📊 Chunked Upload Summary:\n",
      "  • Total chunks: 2\n",
      "  • Successful: 2\n",
      "  • Failed: 0\n",
      "  • Total sensors processed: 6\n",
      "  • Total measurements added: 0\n",
      "  • Total processing time: 8.62s\n",
      "🧹 Cleaning up chunk files from /var/folders/ps/dx2yrk_1117grf32kqlw9qyh0000gq/T/upload_chunks_6ccx_w6c\n",
      "\n",
      "📊 Final Upload Statistics:\n",
      "  • Actual upload time: 8.72 seconds\n",
      "  • Average speed: 0.10 MB/s\n",
      "  • Rows per second: 1028\n"
     ]
    }
   ],
   "source": [
    "# List available files\n",
    "files_info = list_data_files(\"./data/\")\n",
    "\n",
    "# Analyze file sizes\n",
    "sensors_path = \"./data/sensors.csv\"\n",
    "measurements_path = \"./data/measurements.csv\"\n",
    "\n",
    "if os.path.exists(sensors_path) and os.path.exists(measurements_path):\n",
    "    sensors_info = get_file_info(sensors_path)\n",
    "    measurements_info = get_file_info(measurements_path)\n",
    "    \n",
    "    total_size_mb = sensors_info['size_mb'] + measurements_info['size_mb']\n",
    "    total_rows = sensors_info['row_count'] + measurements_info['row_count']\n",
    "    \n",
    "    print(f\"📈 Upload Progress Estimation:\")\n",
    "    print(f\"  • Total data: {total_size_mb:.2f} MB, {total_rows:,} rows\")\n",
    "    print(f\"  • Estimated upload time: {total_size_mb * 2:.1f} seconds\")\n",
    "    \n",
    "    # Start upload with progress tracking\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        result = upload_csv_data_chunked(\n",
    "            campaign_id=campaign_id,\n",
    "            station_id=station_id,\n",
    "            token=token,\n",
    "            data_dir=\"./data/\",\n",
    "            sensors_filename=\"sensors.csv\",\n",
    "            measurements_filename=\"measurements.csv\",\n",
    "            chunk_size=6000,\n",
    "            max_file_size_mb=30\n",
    "        )\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"\\n📊 Final Upload Statistics:\")\n",
    "        print(f\"  • Actual upload time: {total_time:.2f} seconds\")\n",
    "        print(f\"  • Average speed: {total_size_mb/total_time:.2f} MB/s\")\n",
    "        print(f\"  • Rows per second: {total_rows/total_time:.0f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Progress monitored upload failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2fa823",
   "metadata": {},
   "source": [
    "## 7. Best Practices\n",
    "\n",
    "1. **File Preparation:**\n",
    "   - Validate your CSV files before upload\n",
    "   - Ensure sensor aliases match between files\n",
    "   - Use consistent timestamp formats\n",
    "\n",
    "2. **Error Handling:**\n",
    "   - Always wrap API calls in try-catch blocks\n",
    "   - Check file existence before upload\n",
    "   - Validate response status codes\n",
    "\n",
    "3. **Security:**\n",
    "   - Never hardcode credentials in notebooks\n",
    "   - Store tokens securely\n",
    "   - Use environment variables for sensitive data\n",
    "\n",
    "4. **Performance:**\n",
    "   - Keep files under 500 MB for optimal performance\n",
    "   - Use batch uploads for large datasets\n",
    "   - Monitor upload progress and statistics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
